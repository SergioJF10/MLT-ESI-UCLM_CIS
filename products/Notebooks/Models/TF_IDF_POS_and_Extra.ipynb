{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJYG7JEH5ImAVANO6yl8vO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SergioJF10/MLT-ESI-UCLM_CIS/blob/main/products/Notebooks/Models/TF_IDF_POS_and_Extra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF Vectorization, POS Tagging and Extra Features\n",
        "In this colab, we will develop the fourth and final approach, including the TF-IDF vectorization, with POS tagging and two extra features:\n",
        "1. Number of words in an opinion.\n",
        "2. Number of sentences in an opinion.\n",
        "\n",
        "Again, we want to highlight that we did not include the N-grams due to its high memory demanding aspect."
      ],
      "metadata": {
        "id": "8JEutNaWNkU2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wUsgHuDSJxmQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import VotingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!cp '/content/gdrive/MyDrive/Colab Notebooks/Machine Learning Tecniques/Natural Language Processing/products.csv' 'products.csv'\n",
        "!cp '/content/gdrive/MyDrive/Colab Notebooks/Machine Learning Tecniques/Natural Language Processing/x_train.json' 'x_train.json'\n",
        "!cp '/content/gdrive/MyDrive/Colab Notebooks/Machine Learning Tecniques/Natural Language Processing/y_train.json' 'y_train.json'\n",
        "!cp '/content/gdrive/MyDrive/Colab Notebooks/Machine Learning Tecniques/Natural Language Processing/x_test.json' 'x_test.json'\n",
        "!cp '/content/gdrive/MyDrive/Colab Notebooks/Machine Learning Tecniques/Natural Language Processing/y_test.json' 'y_test.json'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qtyY-gOOM0T",
        "outputId": "5bf1bc42-03e8-4736-b12e-cb4a81d336cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Loading the Data\n",
        "From the Preprocessing notebook, we obtain the following files with the data ready to be vectorized.\n",
        "- x_train.json\n",
        "- x_test.json\n",
        "- y_train.json\n",
        "- y_test.json\n",
        "- word_count.json\n",
        "- sents_count.json\n",
        "\n",
        "_Note: Please upload those four files. They can be found in the \"Data/Interim\" folder in the `products` project folder._"
      ],
      "metadata": {
        "id": "bD2vRSpH0NhC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "M0oHR73Tzw4h"
      },
      "outputs": [],
      "source": [
        "x_train = []\n",
        "x_test = []\n",
        "y_train = []\n",
        "y_test = []\n",
        "word_count = []\n",
        "sents_count = []\n",
        "with open('x_train.json', 'r', encoding='utf-8') as x_train_file:\n",
        "  x_train = json.load(x_train_file)\n",
        "with open('x_test.json', 'r', encoding='utf-8') as x_test_file:\n",
        "  x_test = json.load(x_test_file)\n",
        "with open('y_train.json', 'r', encoding='utf-8') as y_train_file:\n",
        "  y_train = json.load(y_train_file)\n",
        "with open('y_test.json', 'r', encoding='utf-8') as y_test_file:\n",
        "  y_test = json.load(y_test_file)\n",
        "with open('word_count.json', 'r', encoding='utf-8') as word_file:\n",
        "  word_count = json.load(word_file)\n",
        "with open('sents_count.json', 'r', encoding='utf-8') as sent_file:\n",
        "  sent_count = json.load(sent_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the file descriptors have been used, we will delete them to save RAM."
      ],
      "metadata": {
        "id": "Y1Z0USzQCiSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del x_test_file\n",
        "del x_train_file\n",
        "del y_test_file\n",
        "del y_train_file\n",
        "del word_file\n",
        "del sent_file"
      ],
      "metadata": {
        "id": "r6jhJz75CSg5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another step is to split correctly the word and sentence count arrays. If we remember, in the NLP [preprocessing colab](https://colab.research.google.com/github/SergioJF10/MLT-ESI-UCLM_CIS/blob/main/products/Notebooks/NLP/NLP_products.ipynb#scrollTo=WnzTadlOEGas) we used the [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) with `shuffle = False`, which means that the data will be returned in the same order as input but splitted.\n",
        "\n",
        "So now, we can just split the arrays with the code above and we have obtained the train and test arrays in the correct order to be matched with X data."
      ],
      "metadata": {
        "id": "2C_h3YCmULEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_train = word_count[:len(x_train)]\n",
        "word_test = word_count[len(x_train):]\n",
        "\n",
        "sent_train = sent_count[:len(x_train)]\n",
        "sent_test = sent_count[len(x_train):]"
      ],
      "metadata": {
        "id": "Y5-AqX71TKPM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. TF-IDF Vectorization, POS Tagging and Extra Features\n",
        "Let's now apply vectorization techniques over the preprocessed data in order to prepare the input for the models."
      ],
      "metadata": {
        "id": "4qKAT5_zbelz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "McO0FRNLbqqt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}